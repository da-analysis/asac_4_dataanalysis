# Databricks notebook source
# MAGIC %md
# MAGIC ## ê°ì„± ë¶„ì„ (ëª¨ë“  ë°ì´í„° í•©ì³ì„œ)

# COMMAND ----------

from pyspark.sql import SparkSession
import pyspark.pandas as ps
import matplotlib.pyplot as plt

review_text = spark.read.table("asac.senti_review_text")
review_text = ps.DataFrame(review_text)

# COMMAND ----------

display(review_text)

# COMMAND ----------



# COMMAND ----------

# MAGIC %md
# MAGIC ### 1. vivekn ëª¨ë¸ ê°ì„± ë¶„ì„
# MAGIC - https://sparknlp.org/2021/11/22/sentiment_vivekn_en.html

# COMMAND ----------

import sparknlp
from sparknlp.base import *
from sparknlp.annotator import *
from pyspark.ml import Pipeline

from pyspark.sql import SparkSession
from pyspark.sql.functions import explode

# COMMAND ----------

review_text.createOrReplaceTempView("review_text")

# COMMAND ----------

display(review_text)

# COMMAND ----------

document = DocumentAssembler() \
.setInputCol("reviewText") \
.setOutputCol("document")

token = Tokenizer() \
.setInputCols(["document"]) \
.setOutputCol("token")

normalizer = Normalizer() \
.setInputCols(["token"]) \
.setOutputCol("normal")

vivekn =  ViveknSentimentModel.pretrained() \
.setInputCols(["document", "normal"]) \
.setOutputCol("result_sentiment")

finisher = Finisher() \
.setInputCols(["result_sentiment"]) \
.setOutputCols("final_sentiment")

pipeline = Pipeline().setStages([document, token, normalizer, vivekn, finisher])

# COMMAND ----------

pipelineModel = pipeline.fit(review_text)
result = pipelineModel.transform(review_text)

# COMMAND ----------

display(result)

# COMMAND ----------

result_vivekn = result.withColumn("final_sentiment", explode('final_sentiment'))
display(result_vivekn)

# COMMAND ----------

# MAGIC %md
# MAGIC -> í…ìŠ¤íŠ¸ ìˆëŠ”ë°ë„, ê²°ê³¼ê°€ naê°€ ë‚˜ì˜¤ê¸°ë„í•¨

# COMMAND ----------

# MAGIC %md
# MAGIC ë¸íƒ€í…Œì´ë¸”ë¡œ ì €ì¥í•˜ê¸°

# COMMAND ----------

num_partitions = 4
name = "asac.senti_vivekn_fin"

result_vivekn.repartition(num_partitions).write.saveAsTable(name, mode="overwrite")

# COMMAND ----------

# MAGIC %sql
# MAGIC select * from asac.senti_vivekn_fin

# COMMAND ----------

vivekn_df = spark.read.table("asac.senti_vivekn_fin")
vivekn_df = ps.DataFrame(vivekn_df)

# COMMAND ----------



# COMMAND ----------



# COMMAND ----------

# MAGIC %md
# MAGIC ### 2. í—ˆê¹…í˜ì´ìŠ¤ transformer í™œìš© => sparkNlpì— ìˆëŠ”ê±° ìš°ì„  ë¨¼ì € í™œìš©
# MAGIC - https://sparknlp.org/2021/11/03/bert_sequence_classifier_multilingual_sentiment_xx.html 
# MAGIC - https://huggingface.co/nlptown/bert-base-multilingual-uncased-sentiment?text=I+like+you.+I+love+you 
# MAGIC - ìŠ¤íƒ€, ì ìˆ˜ ì¶”ì¶œí•´ì„œ í”¼ì³ë¡œ ë§Œë“¤ê¸°
# MAGIC - 1 starëŠ” ë§¤ìš° ë¶€ì •ì , 3starsëŠ” ì¤‘ë¦½, 5starsëŠ” ë§¤ìš° ê¸ì •ì 
# MAGIC - scoreëŠ” í•´ë‹¹ê°ì •ì— ì†í•  í™•ë¥ 
# MAGIC
# MAGIC - ì‘ì—…ì´ ì˜¤ë˜ê±¸ë¦¬ë©´ mulitprocessing ë¼ì´ë¸ŒëŸ¬ë¦¬ ì°¸ê³ í•´ì„œ ì½”ë“œ ìˆ˜ì •í•´ì„œ ëŒë ¤ë³´ê¸° (or joblib)
# MAGIC - ì…ë ¥ë°ì´í„° ì¡°ê±´ì´ ìˆìŒ... 512....ì´ê±° ì˜ë¼ì„œ í• ì§€??
# MAGIC - showëŠ” ë˜ëŠ”ë°, display ì•ˆë˜ëŠ” ê²½ìš°

# COMMAND ----------

import sparknlp
from sparknlp.base import *
from sparknlp.annotator import *
from pyspark.ml import Pipeline

from pyspark.sql import SparkSession
from pyspark.sql.functions import explode

# COMMAND ----------

name3 = "asac.senti_review_text"
review_text_df = spark.read.table(name3)

# COMMAND ----------

review_text_df_10 = review_text_df.limit(10)

# COMMAND ----------

document_assembler = DocumentAssembler() \
    .setInputCol('reviewText') \
    .setOutputCol('document')

tokenizer = Tokenizer() \
    .setInputCols(['document']) \
    .setOutputCol('token')

sequenceClassifier = BertForSequenceClassification \
      .pretrained('bert_sequence_classifier_multilingual_sentiment', 'xx') \
      .setInputCols(['token', 'document']) \
      .setOutputCol('class') \
      .setCaseSensitive(False) \
      .setMaxSentenceLength(512)

pipeline = Pipeline(stages=[
    document_assembler,
    tokenizer,
    sequenceClassifier
])

# COMMAND ----------

result = pipeline.fit(review_text_df).transform(review_text_df)

# COMMAND ----------

#num_partitions = 4
name2 = "asac.senti_trans"

result.write.saveAsTable(name2, mode="overwrite")

# COMMAND ----------

display(result)

# COMMAND ----------

display(result)

# COMMAND ----------



# COMMAND ----------

import sparknlp
from sparknlp.base import *
from sparknlp.annotator import *
from pyspark.ml import Pipeline

from pyspark.sql import SparkSession
from pyspark.sql.functions import explode

# COMMAND ----------

name3 = "asac.senti_review_text"
review_text_df = spark.read.table(name3)

# COMMAND ----------

review_text_df_10 = review_text_df.limit(10)

# COMMAND ----------

document_assembler = DocumentAssembler() \
    .setInputCol('reviewText') \
    .setOutputCol('document')

tokenizer = Tokenizer() \
    .setInputCols(['document']) \
    .setOutputCol('token')

sequenceClassifier = BertForSequenceClassification \
      .pretrained('bert_sequence_classifier_multilingual_sentiment', 'xx') \
      .setInputCols(['token', 'document']) \
      .setOutputCol('class') \
      .setCaseSensitive(False) \
      .setMaxSentenceLength(512)

pipeline = Pipeline(stages=[
    document_assembler,
    tokenizer,
    sequenceClassifier
])

# COMMAND ----------

result = pipeline.fit(review_text_df).transform(review_text_df)

# COMMAND ----------

display(result)

# COMMAND ----------

name = "asac.senti_trans_1000"
result_trans.limit(1000).write.saveAsTable(name)

# COMMAND ----------



# COMMAND ----------

# MAGIC %md
# MAGIC ë©€í‹°í”„ë¡œì„¸ì‹± 100ê°œì”© 10ë²ˆ í•´ë³´ê¸°

# COMMAND ----------

name3 = "asac.senti_review_text"
review_text_df = spark.read.table(name3)

# COMMAND ----------

mport sparknlp
from sparknlp.base import *
from sparknlp.annotator import *
from pyspark.ml import Pipeline

from pyspark.sql import SparkSession
from pyspark.sql.functions import explode
from multiprocessing import Pool


# íŒŒì´í”„ë¼ì¸ ì„¤ì •
document_assembler = DocumentAssembler() \
    .setInputCol('reviewText') \
    .setOutputCol('document')

tokenizer = Tokenizer() \
    .setInputCols(['document']) \
    .setOutputCol('token')

sequenceClassifier = BertForSequenceClassification \
      .pretrained('bert_sequence_classifier_multilingual_sentiment', 'xx') \
      .setInputCols(['token', 'document']) \
      .setOutputCol('class') \
      .setCaseSensitive(False) \
      .setMaxSentenceLength(512)

pipeline = Pipeline(stages=[
    document_assembler,
    tokenizer,
    sequenceClassifier
])

result = pipeline.fit(review_text_df).transform(review_text_df)




# COMMAND ----------

import multiprocessing
multiprocessing.cpu_count()

# COMMAND ----------

!pip install ray
!pip install torch
!pip install transformers
!pip install scikit-learn
!pip install psutil

# COMMAND ----------

import ray
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, explode
from pyspark.ml import Pipeline
from sparknlp.annotator import DocumentAssembler, Tokenizer, BertForSequenceClassification

ray.shutdown()

# Initialize Ray
ray.init()

# Define a function to apply the pipeline on a chunk of data
@ray.remote
def process_chunk(chunk):
    # Initialize Spark session
    spark = SparkSession.builder \
        .appName("SparkNLP Pipeline") \
        .getOrCreate()
    
    document_assembler = DocumentAssembler() \
        .setInputCol('reviewText') \
        .setOutputCol('document')

    tokenizer = Tokenizer() \
        .setInputCols(['document']) \
        .setOutputCol('token')

    sequenceClassifier = BertForSequenceClassification \
          .pretrained('bert_sequence_classifier_multilingual_sentiment', 'xx') \
          .setInputCols(['token', 'document']) \
          .setOutputCol('class') \
          .setCaseSensitive(False) \
          .setMaxSentenceLength(512)

    pipeline = Pipeline(stages=[document_assembler, tokenizer, sequenceClassifier])

    result = pipeline.fit(chunk).transform(chunk)
    
    # Stop Spark session
    spark.stop()
    
    return result

if __name__ == '__main__':
    # Assume review_text_df is the DataFrame containing the data

    # Split the DataFrame into chunks (e.g., 100 rows per chunk)
    chunks = [chunk for chunk in review_text_df.randomSplit([1.0]*100)]

    # Apply the pipeline on each chunk using multiple CPU cores in parallel
    processed_chunks = ray.get([process_chunk.remote(chunk) for chunk in chunks])

    # Concatenate the processed chunks back into a single DataFrame
    result_trans = processed_chunks[0]
    for chunk in processed_chunks[1:]:
        result_trans = result_trans.union(chunk)

    # Save the result DataFrame
    name = "asac.senti_trans_temp"
    result_trans.write.saveAsTable(name)

    # Shutdown Ray
    ray.shutdown()


# COMMAND ----------

#######################################
import sparknlp
from sparknlp.base import *
from sparknlp.annotator import *
from pyspark.ml import Pipeline

from pyspark.sql import SparkSession
from pyspark.sql.functions import explode
from multiprocessing import Pool


# íŒŒì´í”„ë¼ì¸ ì„¤ì •
document_assembler = DocumentAssembler() \
    .setInputCol('reviewText') \
    .setOutputCol('document')

tokenizer = Tokenizer() \
    .setInputCols(['document']) \
    .setOutputCol('token')

sequenceClassifier = BertForSequenceClassification \
      .pretrained('bert_sequence_classifier_multilingual_sentiment', 'xx') \
      .setInputCols(['token', 'document']) \
      .setOutputCol('class') \
      .setCaseSensitive(False) \
      .setMaxSentenceLength(512)


pipeline = Pipeline(stages=[document_assembler, tokenizer, sequenceClassifier])

spark = SparkSession.builder \
    .appName("Multiprocessing Example") \
    .getOrCreate()

# ë³‘ë ¬ ì²˜ë¦¬ í•¨ìˆ˜ ì •ì˜
def process_chunk(chunk):
    result_chunk = pipeline.fit(chunk).transform(chunk)
    return result_chunk

if __name__ == '__main__':
    # review_text_dfëŠ” ì²˜ë¦¬í•  ë°ì´í„°í”„ë ˆì„
    chunks = [review_text_df.limit(10) for _ in range(10)]  # 10ê°œì”© 10ë²ˆ ì²˜ë¦¬í•˜ë„ë¡ ì„¤ì •

    # multiprocessingì„ ì‚¬ìš©í•˜ì—¬ ë³‘ë ¬ë¡œ ì²˜ë¦¬
    with Pool() as pool:
        processed_chunks = pool.map(process_chunk, chunks)

    for i, result_chunk in enumerate(processed_chunks):
        result_chunk.write.saveAsTable(f"asac.senti_trans_multi_{i}", mode="overwrite")

    spark.stop()



# COMMAND ----------

review_text_df_10 = review_text_df.limit(10)

# COMMAND ----------

result = pipeline.fit(review_text_df_10).transform(review_text_df_10)

# COMMAND ----------

display(result)

# COMMAND ----------

# MAGIC %md
# MAGIC íŒë‹¤ìŠ¤ ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ë°”ê¿”ì„œ í•´ë³´ê¸°

# COMMAND ----------

import pandas as pd
name3 = "asac.senti_review_text"
review_text_df = spark.read.table(name3)

# Spark ë°ì´í„°í”„ë ˆì„ì„ íŒë‹¤ìŠ¤ ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ë³€í™˜
review_text_pd_df = review_text_df.toPandas()


# COMMAND ----------

review_text_pd_df.info()

# COMMAND ----------

review_text_df

# COMMAND ----------

name = "asac.senti_trans_1000"
result_trans.limit(1000).write.saveAsTable(name)

# COMMAND ----------

review_text_pd_df_100 = review_text_pd_df[:100]

# COMMAND ----------

review_text_pd_df_100.head(5)

# COMMAND ----------

from pyspark.sql import SparkSession
from joblib import Parallel, delayed
import pandas as pd
from pyspark.ml import Pipeline
from pyspark.ml.feature import Tokenizer
from sparknlp.document import DocumentAssembler
from sparknlp.base import BertForSequenceClassification

# ë³‘ë ¬ ì²˜ë¦¬ë¥¼ ìˆ˜í–‰í•  Python í•¨ìˆ˜ ì •ì˜
def process_row(row):
    document_assembler = DocumentAssembler() \
        .setInputCol('reviewText') \
        .setOutputCol('document')

    tokenizer = Tokenizer() \
        .setInputCols(['document']) \
        .setOutputCol('token')

    sequenceClassifier = BertForSequenceClassification \
        .pretrained('bert_sequence_classifier_multilingual_sentiment', 'xx') \
        .setInputCols(['token', 'document']) \
        .setOutputCol('class') \
        .setCaseSensitive(False) \
        .setMaxSentenceLength(512)

    pipeline = Pipeline(stages=[document_assembler, tokenizer, sequenceClassifier])
    row = pipeline.fit(row).transform(row)

    return row

# Create SparkSession
spark = SparkSession.builder.getOrCreate()

# Apply process_row function to each row using map
processed_data = review_text_pd_df_100.rdd.map(process_row)

# Convert RDD of processed rows to DataFrame
processed_df = processed_data.toDF()


# COMMAND ----------


# ì²˜ë¦¬ ê²°ê³¼ ì €ì¥
processed_df.write.saveAsTable("processed_data_table")

# COMMAND ----------



# COMMAND ----------



# COMMAND ----------



# COMMAND ----------

# MAGIC %pip install --upgrade pip
# MAGIC !pip install --upgrade pip
# MAGIC !pip install huggingface_hub
# MAGIC %pip install spark-nlp
# MAGIC %pip install torch torchvision
# MAGIC !pip install torch torchvision torchaudio
# MAGIC %pip install transformers
# MAGIC %pip install langchain_community
# MAGIC from langchain_community.embeddings import HuggingFaceHubEmbeddings

# COMMAND ----------

!pip install torch torchvision torchaudio

# COMMAND ----------

# MAGIC %pip install torch torchvision

# COMMAND ----------

# MAGIC %pip install transformers
# MAGIC %pip install langchain_community
# MAGIC from langchain_community.embeddings import HuggingFaceHubEmbeddings

# COMMAND ----------

from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline
import torch

# ê°ì • ë¶„ì„ì„ ìœ„í•œ ì‚¬ì „ í›ˆë ¨ëœ ëª¨ë¸ ë¡œë“œ
model_name = "nlptown/bert-base-multilingual-uncased-sentiment"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSequenceClassification.from_pretrained(model_name)

# íŒŒì´í”„ë¼ì¸ ì„¤ì • (ê°ì • ë¶„ì„)
sentiment_analysis_pipeline = pipeline("sentiment-analysis", model=model, tokenizer=tokenizer)

# ë¬¸ì„œ ì˜ˆì‹œ
documents = [
    "I love Hugging Face!", 
    "I hate Hugging Face...",
    "Hugging Face is just okay."
]

# ë¬¸ì„œ ê°ì • ë¶„ì„
for doc in documents:
    result = sentiment_analysis_pipeline(doc)
    print(f"Document: {doc}")
    print(f"Sentiment: {result[0]['label']}, Score: {result[0]['score']}")
    print()


# COMMAND ----------

review_text.show(truncate=False)
review_text.createOrReplaceTempView("review_text")

# COMMAND ----------

from pyspark.sql import SparkSession
from pyspark.sql.functions import udf
from pyspark.sql.types import StringType, ArrayType, StructType, StructField, FloatType

# SparkSession ìƒì„±
spark = SparkSession.builder \
    .appName("Sentiment Analysis") \
    .getOrCreate()

from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline
import torch

# ê°ì • ë¶„ì„ì„ ìœ„í•œ ì‚¬ì „ í›ˆë ¨ëœ ëª¨ë¸ ë¡œë“œ
model_name = "nlptown/bert-base-multilingual-uncased-sentiment"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSequenceClassification.from_pretrained(model_name)

sentiment_analysis_pipeline = pipeline("sentiment-analysis", model=model, tokenizer=tokenizer)

def analyze_sentiment(text):
    result = sentiment_analysis_pipeline(text)
    label = result[0]['label']
    score = result[0]['score']
    return f"Sentiment: {label}, Score: {score}"

# UDF ë“±ë¡
analyze_sentiment_udf = udf(analyze_sentiment, StringType())

# review_text_dfì— UDF ì ìš©(10ê°œë§Œ í•´ë³´ê¸°)
result_df = review_text.limit(10).withColumn("sentiment_analysis", analyze_sentiment_udf("reviewText"))

result_df.show(truncate=False)

# COMMAND ----------

display(result_df)

# COMMAND ----------

def analyze_sentiment(text):
    #text = text[:512]  #  maximum 512 tokens
    result = sentiment_analysis_pipeline(text)
    label = result[0]['label']
    score = result[0]['score']
    return label, score

analyze_sentiment_udf = udf(analyze_sentiment, StructType([
    StructField("Sentiment", StringType(), True),
    StructField("Score", FloatType(), True)
]))
# 10ê°œë§Œ í™•ì¸í•´ë³´ê¸°
result_df = review_text.limit(10).withColumn("sentiment_analysis", analyze_sentiment_udf("reviewText"))
display(result_df)

# COMMAND ----------



# COMMAND ----------

def analyze_sentiment(text):
    #text = text[:512]  #  maximum 512 tokens
    result = sentiment_analysis_pipeline(text)
    label = result[0]['label']
    score = result[0]['score']
    return label, score

analyze_sentiment_udf = udf(analyze_sentiment, StructType([
    StructField("Sentiment", StringType(), True),
    StructField("Score", FloatType(), True)
]))
# 100ê°œë§Œ í™•ì¸í•´ë³´ê¸°
result_df = review_text.limit(100).withColumn("sentiment_analysis", analyze_sentiment_udf("reviewText"))

result_df.show(100)


# COMMAND ----------

display(result_df)

# COMMAND ----------

# MAGIC %md
# MAGIC - í† í°ì´ 512ê°œê°€ ë„˜ëŠ” ë¦¬ë·° ëª‡ê°œ ìˆëŠ”ì§€ í™•ì¸í•˜ê¸°

# COMMAND ----------

from pyspark.sql.functions import col, udf
from pyspark.sql.types import IntegerType
from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline
import torch

# ê°ì • ë¶„ì„ì„ ìœ„í•œ ì‚¬ì „ í›ˆë ¨ëœ ëª¨ë¸ ë¡œë“œ
model_name = "nlptown/bert-base-multilingual-uncased-sentiment"
tokenizer = AutoTokenizer.from_pretrained(model_name)

# UDF ì •ì˜: í† í°í™”í•˜ê³  í† í°ì˜ ê°œìˆ˜ë¥¼ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜
def count_tokens(text):
    tokens = tokenizer.tokenize(text)
    # í† í° ê°œìˆ˜
    return len(tokens)

count_tokens_udf = udf(count_tokens, IntegerType())

review_text_token = review_text_token.withColumn("token_cnt", count_tokens_udf(col("reviewText")))

review_text_token.show(230) #240ì€ ì•ˆë¨

# COMMAND ----------

display(review_text.head(250))

# COMMAND ----------

# MAGIC %md
# MAGIC - 238í–‰ì— nullê°’ì´ ìˆì–´ì„œ ê·¸ëŸ°ê±° ê°™ìŒ.... ì™œ ë„ê°’...?
# MAGIC - ê·¸ëŸ¼ nullê°’ì¸ í–‰ì„ ì‚­ì œí•˜ê³  ë‹¤ì‹œ í•´ë³´ê² 

# COMMAND ----------

review_text_nona = review_text.na.drop(subset=["reviewText"])

# COMMAND ----------

# MAGIC %md
# MAGIC - transformer í† í¬ë‚˜ì´ì € í™œìš©

# COMMAND ----------

from pyspark.sql.functions import col, udf
from pyspark.sql.types import IntegerType, ArrayType

# UDF ì •ì˜: í…ìŠ¤íŠ¸ë¥¼ í† í°í™”í•˜ê³  í† í°ì˜ ê°œìˆ˜ë¥¼ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜
model_name = "nlptown/bert-base-multilingual-uncased-sentiment"
tokenizer = AutoTokenizer.from_pretrained(model_name)
def tokenize_text(text):
    # í…ìŠ¤íŠ¸ë¥¼ í† í°í™”
    tokens = tokenizer.tokenize(text)
    return tokens

tokenize_text_udf = udf(tokenize_text, ArrayType(StringType()))

review_text_token = review_text_nona.withColumn("tokens", tokenize_text_udf(col("reviewText")))


from pyspark.sql.functions import col, udf
from pyspark.sql.types import IntegerType

# UDF ì •ì˜: í† í°í™”í•˜ê³  í† í°ì˜ ê°œìˆ˜ë¥¼ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜
def count_tokens(text):
    tokens = tokenizer.tokenize(text)
    # í† í° ê°œìˆ˜
    return len(tokens)

count_tokens_udf = udf(count_tokens, IntegerType())

review_text_token = review_text_token.withColumn("token_cnt", count_tokens_udf(col("reviewText")))

review_text_token.show(250) 

# COMMAND ----------

# 512ë¥¼ ì´ˆê³¼ ê°œìˆ˜ 
count_over_512 = review_text_token.filter(review_text_token["token_cnt"] > 512).count()
print(count_over_512)


# COMMAND ----------

# MAGIC %md
# MAGIC #### ê·¸ëŸ¼ ê¸°ë³¸í† í¬ë‚˜ì´ì €ì™€ transformerì—ì„œì˜ í† í¬ë‚˜ì´ì €ì— ì°¨ì´ê°€ ìˆëŠ”ì§€ í™•ì¸
# MAGIC #### ì—†ë‹¤ë©´, ê¸°ë³¸ í† ê·¸íƒ€ì´ì €ì—ì„œ 512ë„˜ëŠ” í–‰ì„ ì‚­ì œí›„ ë‹¤ì‹œ ëŒë ¤ë³´ê¸°

# COMMAND ----------

from pyspark.sql.functions import col, udf
from pyspark.sql.types import IntegerType, ArrayType
from sparknlp.annotator import Tokenizer
from sparknlp.base import DocumentAssembler

documentAssembler = DocumentAssembler() \
    .setInputCol("reviewText") \
    .setOutputCol("document")

# Tokenizerë¥¼ ì‚¬ìš©í•˜ì—¬ ë¬¸ì„œë¥¼ í† í°í™”
tokenizer = Tokenizer() \
    .setInputCols(["document"]) \
    .setOutputCol("token")

review_text_nona = review_text.na.drop(subset=["reviewText"])

from pyspark.ml import Pipeline

pipeline = Pipeline(stages=[documentAssembler, tokenizer])
pipeline_model = pipeline.fit(review_text_nona)
tokenized_data = pipeline_model.transform(review_text_nona)

from pyspark.sql.functions import size

# í† í°ì˜ ê°œìˆ˜ë¥¼ ì„¸ëŠ” UDF ì •ì˜
count_tokens_udf = udf(lambda tokens: len(tokens), IntegerType())
tokenized_data = tokenized_data.withColumn("token_cnt", count_tokens_udf(col("token.result")))

tokenized_data.show(100)

# COMMAND ----------

# transformer í† í¬ë‚˜ì´ì €
import matplotlib.pyplot as plt
from transformers import AutoTokenizer
from pyspark.sql.functions import col, udf
from pyspark.sql.types import IntegerType, ArrayType

text_column = review_text_nona.limit(100)


model_name = "nlptown/bert-base-multilingual-uncased-sentiment"
tokenizer = AutoTokenizer.from_pretrained(model_name)
# UDF ì •ì˜: í…ìŠ¤íŠ¸ë¥¼ í† í°í™”í•˜ê³  í† í°ì˜ ê°œìˆ˜ë¥¼ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜
def tokenize_text(text):
    # í…ìŠ¤íŠ¸ë¥¼ í† í°í™”
    tokens = tokenizer.tokenize(text)
    return tokens

tokenize_text_udf = udf(tokenize_text, ArrayType(StringType()))

review_text_token = text_column.withColumn("tokens", tokenize_text_udf(col("reviewText")))


# UDF ì •ì˜: í† í°í™”í•˜ê³  í† í°ì˜ ê°œìˆ˜ë¥¼ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜
def count_tokens(text):
    tokens = tokenizer.tokenize(text)
    # í† í° ê°œìˆ˜
    return len(tokens)

count_tokens_udf = udf(count_tokens, IntegerType())

review_text_token = review_text_token.withColumn("token_cnt", count_tokens_udf(col("reviewText")))

review_text_token.show(100) 

# COMMAND ----------

# MAGIC %md
# MAGIC - ë‘˜ì˜ í† í°í™” ë°©ì‹ì´ ë‹¤ë¦„
# MAGIC - ìš°ì„  ê·¸ëŸ¼ transformer ëª¨ë¸ í™œìš©í•  ë•ŒëŠ”, 512ë„˜ëŠ” í–‰ì— ëŒ€í•´ì„œëŠ” ì‚­ì œí•˜ê¸° ë³´ë‹¤ëŠ” 512ê¹Œì§€ë§Œ textë¥¼ í™œìš©í•˜ëŠ”ê²Œ ë” ë‚˜ì„ê²ƒ ê°™ìŒ
# MAGIC =>ê·¸ëŸ¼ í˜„ì¬ê¹Œì§€ viveknê³¼ì˜ ì°¨ì´ì ì€ ë¦¬ë·°í…ìŠ¤íŠ¸ê°€ naì¸ í–‰ì„ ì‚­ì œí•œ ê²ƒê³¼, í…ìŠ¤íŠ¸ 512ê¹Œì§€ë§Œ í•œ ê²ƒìœ¼ë¡œ ì§„í–‰í•œë‹¤ëŠ” ì !

# COMMAND ----------

from pyspark.sql import SparkSession
from pyspark.sql.functions import udf
from pyspark.sql.types import StringType, ArrayType, StructType, StructField, FloatType

# SparkSession ìƒì„±
spark = SparkSession.builder \
    .appName("Sentiment Analysis") \
    .getOrCreate()


# PyTorch ë° Transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ import
from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline
import torch

# ê°ì • ë¶„ì„ì„ ìœ„í•œ ì‚¬ì „ í›ˆë ¨ëœ ëª¨ë¸ ë¡œë“œ
model_name = "nlptown/bert-base-multilingual-uncased-sentiment"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSequenceClassification.from_pretrained(model_name)

# íŒŒì´í”„ë¼ì¸ ì„¤ì • (ê°ì • ë¶„ì„)
sentiment_analysis_pipeline = pipeline("sentiment-analysis", model=model, tokenizer=tokenizer)

# UDF ì •ì˜
def analyze_sentiment(text):
    text = text[:512]
    result = sentiment_analysis_pipeline(text)
    label = result[0]['label']
    score = result[0]['score']
    return f"Sentiment: {label}, Score: {score}"


analyze_sentiment_udf = udf(analyze_sentiment, StringType())

review_text = spark.read.table("asac.senti_review_text")

result_trans= review_text.withColumn("sentiment_analysis", analyze_sentiment_udf("reviewText"))

# COMMAND ----------

result_trans.show(10,truncate=False)

# COMMAND ----------

# MAGIC %md
# MAGIC - starì™€ scoreì—´ ë”°ë¡œ ë¶„ë¦¬í•´ì„œ ê°€ì ¸ì˜¤ê¸°

# COMMAND ----------

from pyspark.sql.functions import regexp_extract

# Sentiment ì—´ ìƒì„±
result_trans = result_trans.withColumn("sentiment", regexp_extract("sentiment_analysis", r"(\d+) star", 1).cast(IntegerType()))


# Score ì—´ ìƒì„±
result_trans = result_trans.withColumn("score", regexp_extract("sentiment_analysis", r"Score: ([0-9.]+)", 1).cast(FloatType()))

result_trans.show(100)

# COMMAND ----------

result_trans.show(300)

# COMMAND ----------

# MAGIC %md
# MAGIC ë¸íƒ€í…Œì´ë¸”ë¡œ ì €ì¥í•˜ê¸°

# COMMAND ----------

name = "asac.senti_trans10"
result_trans.limit(10).write.saveAsTable(name)

# COMMAND ----------

# MAGIC %sql
# MAGIC select * from asac.senti_trans10

# COMMAND ----------

name = "asac.senti_trans_10"
result_trans.limit(10).write.saveAsTable(name)

# COMMAND ----------

# MAGIC %sql
# MAGIC select * from asac.senti_trans_10

# COMMAND ----------

# MAGIC %sql
# MAGIC drop table asac.senti_trans_10

# COMMAND ----------

name = "asac.senti_trans_100"
result_trans.limit(100).write.saveAsTable(name)

# COMMAND ----------

total_rows = result_trans.count()
total_rows

# COMMAND ----------

from pyspark.sql import functions as F
from pyspark.sql.window import Window

name = "asac.senti_trans_while20"
total_rows = result_trans.limit(10000).count()
batch_size = 100
start_row = 0

while start_row < total_rows:
    # í˜„ì¬ ì €ì¥ëœ í–‰ì˜ ë²”ìœ„ì—ì„œ ë‹¤ìŒ 100ê°œì˜ ë°ì´í„° ì„ íƒ
    batch_df = result_trans.limit(10000).withColumn("row_number", F.row_number().over(Window.orderBy(F.monotonically_increasing_id()))) \
                           .filter(col("row_number").between(start_row + 1, start_row + batch_size))
    
    # ì´ì „ì— ì €ì¥ëœ ë°ì´í„°ë¥¼ ê±´ë„ˆë›°ê³  ë‹¤ìŒ 100ê°œ ë°ì´í„°ë¥¼ ì„ íƒ
    if start_row > 0:
        # ìƒˆë¡œìš´ ë°°ì¹˜ ë°ì´í„°í”„ë ˆì„ì„ ê¸°ì¡´ ë°ì´í„°í”„ë ˆì„ì— ì¶”ê°€
        batch_df = batch_df.select(*result_trans.columns)
    
    # ì„ íƒëœ ë°ì´í„°ë¥¼ í…Œì´ë¸”ì— ì¶”ê°€ë¡œ ì €ì¥
    batch_df.drop("row_number").write.mode("append").saveAsTable(name)
    
    # ë‹¤ìŒ ë°˜ë³µì„ ìœ„í•´ ì‹œì‘ í–‰ì„ ì—…ë°ì´íŠ¸
    start_row += batch_size 


# COMMAND ----------

# MAGIC %sql
# MAGIC select * from asac.senti_trans_while20

# COMMAND ----------

num_partitions = 4
name2 = "asac.senti_trans"

result_trans.repartition(num_partitions).write.saveAsTable(name2, mode="overwrite")

# COMMAND ----------

#name = "asac.senti_trans"
#result_trans.write.saveAsTable(name)

# COMMAND ----------

name = "asac.senti_trans_100"
result_trans.limit(100).write.saveAsTable(name)

# COMMAND ----------

name = "asac.senti_trans_1000"
result_trans.limit(1000).write.saveAsTable(name)

# COMMAND ----------



# COMMAND ----------



# COMMAND ----------

# MAGIC %md
# MAGIC ### vivekn ëª¨ë¸, transformer ëª¨ë¸ íƒìƒ‰í•˜ê¸°
# MAGIC - ê¸ë¶€ì •ê²°ê³¼ vs ë¦¬ë·° í‰ì 
# MAGIC - ì–´ë–¤ ë¶„í¬ë¥¼ ë³´ì´ëŠ” ì§€ í™•ì¸í•˜ê¸°

# COMMAND ----------

# MAGIC %sql
# MAGIC select overall, final_sentiment from asac.senti_vivekn_fin

# COMMAND ----------

# MAGIC %sql
# MAGIC select overall, final_sentiment from asac.senti_vivekn_fin
# MAGIC where final_sentiment=="positive"

# COMMAND ----------

# MAGIC %sql
# MAGIC select overall, final_sentiment from asac.senti_vivekn_fin
# MAGIC where final_sentiment=="negative"

# COMMAND ----------

# MAGIC %sql
# MAGIC SELECT overall, final_sentiment  FROM asac.senti_vivekn_fin
# MAGIC WHERE final_sentiment !="positive" and final_sentiment !="negative"

# COMMAND ----------

# MAGIC %sql
# MAGIC SELECT overall, final_sentiment  FROM asac.senti_vivekn_fin

# COMMAND ----------

# MAGIC %sql
# MAGIC select mean(overall), min(overall), max(overall), median(overall) from asac.senti_vivekn_fin
# MAGIC where final_sentiment=="positive"
# MAGIC group by final_sentiment
# MAGIC

# COMMAND ----------

# MAGIC %sql
# MAGIC select mean(overall), min(overall), max(overall), median(overall) from asac.senti_vivekn_fin
# MAGIC where final_sentiment=="negative"
# MAGIC group by final_sentiment

# COMMAND ----------

# MAGIC %sql
# MAGIC select mean(overall), min(overall), max(overall), median(overall),count(overall) from asac.senti_vivekn_fin
# MAGIC group by final_sentiment
# MAGIC -- 1ì´ positive
# MAGIC -- 2ê°€ negative
# MAGIC -- 3ì´ na

# COMMAND ----------



# COMMAND ----------



# COMMAND ----------



# COMMAND ----------



# COMMAND ----------



# COMMAND ----------

# MAGIC %md
# MAGIC ### 3. ê¸ë¶€ì •ìœ¼ë¡œ ë‚˜ì˜¤ëŠ” ê²ƒ(ê·¸ëƒ¥ ì¶”ê°€ë¡œ í•´ë³¸ê²ƒ, ì–˜ëŠ” ë” ì•ˆí•˜ê¸°)
# MAGIC - ì—¬ê¸°ì„œ scoreëŠ” í•´ë‹¹ ê°ì •ì— ì†í•  í™•ë¥ ì„ ë‚˜íƒ€ëƒ„

# COMMAND ----------

!pip install transformers

# COMMAND ----------

from transformers import pipeline

# COMMAND ----------

!pip install torch torchvision torchaudio

# COMMAND ----------

# MAGIC %pip install torch torchvision

# COMMAND ----------

from transformers import pipeline

# build pipeline
classifier = pipeline("sentiment-analysis")

#inference
classifier("we are very happy to show you the ğŸ¤— Transformers library.")

# COMMAND ----------

from pyspark.sql.functions import udf
from pyspark.sql.types import StringType, FloatType, StructType, StructField

classifier = pipeline("sentiment-analysis")

def analyze_sentiment(text):
    text = text[:512]  #  maximum 512 tokens
    result = classifier(text)[0]
    label = result['label']
    score = result['score']
    return label, score

analyze_sentiment_udf = udf(analyze_sentiment, StructType([
    StructField("label", StringType(), True),
    StructField("score", FloatType(), True)
]))

result_df = review_text.withColumn("sentiment_analysis", analyze_sentiment_udf("reviewText"))

result_df = result_df.withColumn("label", result_df["sentiment_analysis"].getField("label")) \
                     .withColumn("score", result_df["sentiment_analysis"].getField("score")) \
                     .drop("sentiment_analysis")

result_df.show(truncate=False)


# COMMAND ----------

display(result_df.head(100))

# COMMAND ----------


